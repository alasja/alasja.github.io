---
title: "几何阶段"
keywords: sample homepage
tags: [getting_started]
sidebar: rtr
layout: book
permalink: rtr_ch2_geometry.html
summary: 渲染管线。
---



### 2.3 几何阶段
几何阶段复杂大部分逐多边形和逐像素的操作。这个阶段被分为下面的多个功能阶段：模型及视角变换、顶点着色器、投影、裁剪及屏幕映射（见图2.3）。再次提醒，这些功能阶段是基于实现的，而不是必须要存在的。有些情况下，很多连续的功能阶段在一个流水线阶段就完成了。也有可能一个功能阶段被分割成多个流水线阶段。

![图2-3](/images/RTR3.02.03.png)

某种极限情况下，流水线所有阶段可能都运行在一个处理器上，然后你可以说整条流水线就一个流水线阶段。实际上在处理器芯片集成之前，还真是这么搞得。另一种极端的情况是，每个功能阶段都被划分成多个更小的流水线阶段，然后每个小流水线阶段可以运行在特定的处理器核心上。

### 2.3.1 模型和视角变换
一个模型要显示到屏幕上，需要经历几次不同坐标空间的变换。最开始，模型在它自己的模型空间，这时候它还没有经历任何的变换。每个模型都可以关联一个**模型变换**，用于指定位置和朝向。一个模型可以关联多个模型变换。这可以让同一个模型在一个场景中存在多个复制体（叫实例），他们可以分别指定不同的位置和朝向，而不用复制原始模型来达到这个目的。

变换的时候，其实是顶点和法线在变换。模型的坐标系统被称为模型坐标系。在进行模型变换后，这个模型就在世界坐标系或者世界空间下了。世界坐标系是唯一的，不同的模型进行各自的变换后，所有的模型都会在同一个空间下了。

正如之前提到的，只有被相机看到的模型才会渲染。相机也有自己位置和方向用于定位和设置相机的朝向。为了更方便的进行投影和裁剪，相机和所有的模型都会变换到观察坐标系下。观察变换的目标是把相机放到原点并设置它的朝向z轴的负方向，并且y轴朝上，x轴朝右边。观察变换最终的位置和朝向取决于底层API。这个空间也被称为相机空间，或者通俗点叫眼睛空间。图2.4是观察变换如何作用相机和模型的示例。模型变换和相机变换都是4x4的矩阵，在第四章会深入探讨。

![图2-4](/images/RTR3.02.04.png)


### 2.3.2 顶点着色器
要想场景看起来真实，不仅要画出对象的形状和位置，模型的表现也得跟上才行。这个表现就包括对象的材质，灯光效果等。材质和灯光的模拟有很多方式，从单色到精美的物理仿真都有。

着色器就是用来决定灯光如何在材质上作用的操作。它对模型上的各个点进行着色计算。这些计算大部分在几何阶段完成，有些也会在栅格阶段的逐像素操作是完成。一个顶点能保存多种数据，比如位置、法线、颜色以及其他着色器需要的数值。顶点着色器的计算结果（包括颜色、向量、纹理坐标等等）会被传递给栅格阶段，并被自动插值。

着色计算一般在世界空间下完成。但是在实践中，有些时候为了方便计算，也会把相关的元素转换到其他空间（如相机空间）去计算。这是因为只要在同一个空间下，相机、光源、对象的相对关系就是一致的。

第三章和第五章会更深入的讨论着色器。


### 2.3.3 投影
着色器之后，渲染系统就要开始投影了，投影就是说把整个可视范围转换到一个单位立方体里面，范围是从(-1,-1,-1)到(1,1,1)。这个单位立方体叫做规范化视体。有两种场景的投影方式，透视投影和平视或平行投影。见图2.5

![图2-5](/images/RTR3.02.05.png)

平视透视的可视范围就是普通的长方体，它的投影变换就是把这个长方体变换成规范化视体。平视投影的主要特点是变换是线性的，不会把平行线变的不平行。这个变换包括平移和缩放。

透视投影就比较复杂一点。在这个投影下，离相机越远，投影之后就越小。平行线在投影之后看起来会在远处相交。这个投影模拟我们人眼观察物体的方式。我们把可视范围叫做<font color="tomato">平截头体</font>，它的几何形状是个被截断的金字塔形。平截头体也会被变换到规范化视体。两种投影都可用4x4的矩阵来描述，通过变换之后，我们就可以说模型在规范化设备坐标系下。

之所以叫投影，不是因为矩阵变换把一个空间变到另一空间，而是图像在显示之后，Z坐标就被丢弃了，也是就是模型从3D投影成了2D的。

### 2.3.4 裁剪
完全在可视范围内的图元会进入到栅格阶段，并会绘制到屏幕上。也只有完全可视的图元才有下一个阶段。完全在可视范围之外的图元不会被渲染。只有那些部分在里面的图元需要进行裁剪。举个例子，一个线段一点里面，一点在外面，会被视椎体裁剪，在外面的顶点被替换成线段跟视椎体的交点。使用投影矩阵变换之后，图元要跟规范化视体做裁剪。在裁剪之前做空间变化和投影的好处是裁剪部分比较统一，图元永远都是跟规范化视体做裁剪。图2.6展示了裁剪过程。除了视椎体的六个面之外，用户还可以定义其他平面来做裁剪。这种方式显示的图像被称为切片，参考章节14.1。与前面几个几何阶段不同的是，裁剪没法通过编程修改，它一般在固定功能硬件上实现。

![图2-6](/images/RTR3.02.06.png)


### 2.3.5 屏幕映射
在视椎体内（以及被裁剪过的）的图元都会进入到这个阶段，开始的时候，坐标系依旧是三维的。xy坐标会被变换成屏幕坐标。屏幕坐标及z坐标合称为窗口坐标。假设一个屏幕能显然的坐标最小为$(x_1,y_1)$，最大为$(x_2,y_2)$且$x_1 \lt x_2, y_1 \lt y_2$。那么屏幕映射可能会做一个缩放。Z坐标在映射中没有什么作用。新的xy坐标在屏幕坐标系下，然后连同Z坐标，一起传递给栅格阶段。图2.7展示了屏幕映射过程。

![图2-7](/images/RTR3.02.07.png)

一个困惑的地方是整型和浮点型的坐标值如何对应到像素坐标（或者纹理坐标）的。在DX9.0中，把0.0当做像素的中心，意思是像素[0,9]所占的范围是[-0.5,9.5]。Hechbert[520]提出了一个更合理的方式。给定一些水平的像素并使用笛卡尔坐标系的情况下，最左边的像素点的左边界的浮点值是0.0。OpenGL使用的就是这种方式，DX10及后续也使用这种方式。像素的中心是0.5，所以像素[0,9]的实际范围是[0.0, 10.0]。两者的转换很简单：

<font size="4">$$
d = floor(c), \tag{2.1} 
$$</font>
<font size="4">$$
c = d + 0.5,  \tag{2.2} 
$$</font>

这里d是像素的索引，而c则是浮点值。

虽然所有API中的横坐标值都是从左到右增大的，但是DX跟OpenGL的纵坐标方向却不一致。OpenGL使用笛卡尔坐标系，把左下角作为最小点。而DX有时候使用右上作为最小点，根据内容而变。这两种方式没有谁对谁错，都是有道理可讲的。举例来说，在OpenGL中图片的(0,0)点是左下角，DX则是在左上角。DX使用这种方式是因为很多东西是从上到下表现的，比如：微软Windows系统使用这种坐标系统，我们阅读也是这种方式，很多图片文件存储数据的方式也是这样。所以关键是他们之间存在差异，如果移植相关的API则需要重点关注这一点。






